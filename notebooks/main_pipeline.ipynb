{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-07T11:54:44.387109300Z",
     "start_time": "2024-06-07T11:54:43.967958600Z"
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import replicate\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = os.getenv(\"REPLICATE_API_TOKEN\")\n",
    "\n",
    "def get_user_intent(query, image_path):\n",
    "    image = open(image_path, \"rb\")\n",
    "    \n",
    "    json_output = {\n",
    "        'user_intent': {\n",
    "            'action': \"What the user wants to do. One word. (enum: 'buy', 'sell')\",\n",
    "        },\n",
    "        'items': [\n",
    "            {\n",
    "                'name': 'Name of the item',\n",
    "                'type': \"Type of the item (enum: 'furniture', 'clothing', 'electronics', 'food', 'other')\",\n",
    "                'description': 'Extremely detailed description of the item. DO NOT relate the description to other objects.',\n",
    "                'bounding_box': '(x1, y1, height, width)',\n",
    "            },\n",
    "            {\n",
    "                'name': 'Name of the second item... and so on'\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    input = {\n",
    "        \"image\": image,\n",
    "        \"prompt\": f\"Please interpret the query to determine the users intent. Then, use the image to further determine what the user wants. If there is no image, then output solely based on the given text input. If the user is specific (asking for all items in the image is specific) or there is only one item in the image, please give as much separate detail about each item as possible. DO NOT include any other commentary. Please format your response in a json format: {json.dumps(json_output)}. Query: {query}\",\n",
    "    }\n",
    "    \n",
    "    output = replicate.run(\n",
    "        \"yorickvp/llava-v1.6-34b:41ecfbfb261e6c1adf3ad896c9066ca98346996d7c4045c5bc944a79d430f174\",\n",
    "        # \"yorickvp/llava-13b:b5f6212d032508382d61ff00469ddda3e32fd8a0e75dc39d8a4191bb742157fb\",\n",
    "        input=input\n",
    "    )\n",
    "    output = \"\".join(output)\n",
    "    # convert the output to a json object\n",
    "    json_output = json.loads(output.strip().replace('\\n', '').replace('`', '').replace('json{', '{'))\n",
    "    return json_output"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vio/miniconda3/envs/MultiOn/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/vio/miniconda3/envs/MultiOn/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Initialize Faster R-CNN\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Function to detect objects using Faster R-CNN\n",
    "def detect_objects(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "    image_tensor = transform(image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(image_tensor)\n",
    "    \n",
    "    return image, predictions[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T13:34:08.438440900Z",
     "start_time": "2024-06-07T13:34:08.029285Z"
    }
   },
   "id": "67181ef9d2e977f7",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to crop an object from the image\n",
    "def crop_object(image, box):\n",
    "    return image.crop((int(box[0]), int(box[1]), int(box[2]), int(box[3])))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T11:54:56.026918200Z",
     "start_time": "2024-06-07T11:54:55.981153600Z"
    }
   },
   "id": "8e417abda50c711f",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to describe the object using LLaVA\n",
    "# I wanted to use gpt, but I forgot that Images was deprecated\n",
    "def describe_object_gpt(cropped_image):    \n",
    "    input = {\n",
    "        \"image\": encode_image_to_base64(cropped_image),\n",
    "        \"prompt\": \"Please describe the object in the image in as much detail as possible. DO NOT include any other commentary.\",\n",
    "    }\n",
    "\n",
    "    output = replicate.run(\n",
    "        \"yorickvp/llava-v1.6-34b:41ecfbfb261e6c1adf3ad896c9066ca98346996d7c4045c5bc944a79d430f174\",\n",
    "        # \"yorickvp/llava-v1.6-vicuna-13b:0603dec596080fa084e26f0ae6d605fc5788ed2b1a0358cd25010619487eae63\",\n",
    "        input=input\n",
    "    )\n",
    "    output = \"\".join(output)\n",
    "    return str(output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T13:15:08.797322200Z",
     "start_time": "2024-06-07T13:15:08.774167600Z"
    }
   },
   "id": "dae206ec77666c2a",
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import io\n",
    "# Function to encode the image to base64 for the MultiOn API\n",
    "def encode_image_to_base64(image):\n",
    "\n",
    "    # Save the image as cropped_object_1.jpg\n",
    "    image.save(\"cropped_object1.jpg\")\n",
    "    \n",
    "    # Open it to get the Binary IO\n",
    "    return open(\"cropped_object1.jpg\", \"rb\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T11:54:58.910484900Z",
     "start_time": "2024-06-07T11:54:58.833958400Z"
    }
   },
   "id": "1fadc69ba0eff427",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Uploading the image to imgur\n",
    "import requests\n",
    "\n",
    "def upload_image(image_path, image_name):\n",
    "    url = \"https://api.imgur.com/3/image\"\n",
    "    \n",
    "    payload={'type': 'image',\n",
    "    'title': 'Simple upload',\n",
    "    'description': 'This is a simple image upload in Imgur'}\n",
    "    files=[\n",
    "      ('image',(image_name,encode_image_to_base64(image_path),'image/jpeg'))\n",
    "    ]\n",
    "    headers = {\n",
    "      'Authorization': 'Client-ID {{clientId}}'\n",
    "    }\n",
    "    \n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n",
    "\n",
    "    return response.json()['data']['link']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T11:55:00.103590100Z",
     "start_time": "2024-06-07T11:54:59.983508700Z"
    }
   },
   "id": "26ab07dc5b42a173",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from multion.client import MultiOn\n",
    "\n",
    "multion = MultiOn(api_key=os.getenv(\"MULTION_API_KEY\"))\n",
    "\n",
    "def search_multion(description, image_path=None):\n",
    "    # Use MultiOn API to search for the object\n",
    "\n",
    "    if image_path:\n",
    "        image_url = upload_image(image_path, \"cropped_object.jpg\")\n",
    "        url = \"https://images.google.com/\"\n",
    "        cmd = f\"Go to this url: https://images.google.com/. Click on the search by image button. In the paste image link here box, input the url: {image_url}. Please find the best matching image url and site url based on the initial given image (left half of the page when you load the results. DO NOT OUTPUT THE GIVEN URL) and it's price. Output a SINGLE tuple (url, image_url) and no other text or commentary. If at any time you are redirected to the main google search page, please restart the process. DO NOT PASTE THE URL INTO THE SEARCH BAR.\"\n",
    "    else:\n",
    "        url = \"https://www.google.com/\"\n",
    "        cmd = f\"Please find the best matching image url and site url based on the following description and it's price. Output a SINGLE tuple (url, image_url) and no other text or commentary. Description: {description}\"\n",
    "    \n",
    "    response = multion.browse(\n",
    "        cmd=cmd,\n",
    "        url=url,\n",
    "        local=True\n",
    "    )\n",
    "    \n",
    "    return response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T12:46:48.449315700Z",
     "start_time": "2024-06-07T12:46:48.331662200Z"
    }
   },
   "id": "baeff9ce6efd802a",
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vio/miniconda3/envs/MultiOn/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/vio/miniconda3/envs/MultiOn/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "2024-06-07 06:33:58.756710: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-07 06:33:59.513486: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/vio/miniconda3/envs/MultiOn/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/vio/miniconda3/envs/MultiOn/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Load CLIP model and processor\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(model_name)\n",
    "clip_processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Function to get image embeddings using CLIP\n",
    "def get_image_embedding(image):\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        embeddings = clip_model.get_image_features(**inputs)\n",
    "    return embeddings\n",
    "\n",
    "# Function to get similarity between two image embeddings\n",
    "def calculate_similarity(embedding1, embedding2):\n",
    "    # Normalize the embeddings\n",
    "    embedding1 = embedding1 / embedding1.norm(p=2, dim=-1, keepdim=True)\n",
    "    embedding2 = embedding2 / embedding2.norm(p=2, dim=-1, keepdim=True)\n",
    "    # Cosine similarity\n",
    "    similarity = torch.matmul(embedding1, embedding2.T).item()\n",
    "    return similarity\n",
    "\n",
    "# Function to verify the best match using CLIP\n",
    "def verify_image_similarity(cropped_image, search_results):\n",
    "    cropped_embedding = get_image_embedding(cropped_image)\n",
    "    best_match = None\n",
    "    highest_similarity = 0\n",
    "\n",
    "    for result in search_results:\n",
    "        try:\n",
    "            response = requests.get(result)\n",
    "            if response.status_code == 200:\n",
    "                search_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "                search_embedding = get_image_embedding(search_image)\n",
    "                similarity = calculate_similarity(cropped_embedding, search_embedding)\n",
    "                if similarity > highest_similarity:\n",
    "                    highest_similarity = similarity\n",
    "                    best_match = result\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return best_match"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T13:34:01.071246300Z",
     "start_time": "2024-06-07T13:33:56.321353300Z"
    }
   },
   "id": "31ee8e7a1e962a8",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def feedback_loop(cropped_image, initial_results, feature_extractor, preprocess, max_retries=3):\n",
    "    best_match = verify_image_similarity(cropped_image, initial_results)\n",
    "    retries = 0\n",
    "    print(f\"Feedback (#{retries}: {best_match}\")\n",
    "    while best_match is None and retries < max_retries:\n",
    "        retries += 1\n",
    "        new_results = search_multion(describe_object_gpt(cropped_image))\n",
    "        best_match = verify_image_similarity(cropped_image, new_results)\n",
    "    return best_match"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T12:39:08.652933400Z",
     "start_time": "2024-06-07T12:39:08.616854300Z"
    }
   },
   "id": "683e1e7680d1b4be",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Please buy the couch and the table for me.\n",
      "Best match for couch: https://www.kardiel.com/swoosh-modular-60-2-seater-arm-left-fossil-velvet/\n",
      "Best match for table: https://www.cgtrader.com/3d-models/furniture/table/cylinder-dining-table-set-by-davidson-london\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "\n",
    "image_path = \"imgs/furniture1.jpg\"\n",
    "query = \"Please buy the couch and the table for me.\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Deciphering user intent\n",
    "json_output = get_user_intent(query, image_path)\n",
    "\n",
    "# Object Detection\n",
    "image, predictions = detect_objects(image_path)\n",
    "predicted_boxes = predictions['boxes']\n",
    "predicted_labels = predictions['labels']\n",
    "decoded_labels = [weights.meta[\"categories\"][label] for label in predicted_labels]\n",
    "predicted_scores = predictions['scores']\n",
    "\n",
    "# Processing the user intent\n",
    "user_intent = json_output['user_intent']['action']\n",
    "items = json_output['items']\n",
    "\n",
    "if user_intent == \"buy\":\n",
    "    for item in items:\n",
    "        print(f\"Processing item: {item['name']}\")\n",
    "\n",
    "        # Find the corresponding box for the item\n",
    "        item_box = None\n",
    "        for box, label in zip(predicted_boxes, decoded_labels):\n",
    "            if item['name'].lower() in label.lower():\n",
    "                item_box = box\n",
    "                break\n",
    "\n",
    "        if item_box is not None:\n",
    "            cropped_image = crop_object(image, item_box)\n",
    "            description = describe_object_gpt(cropped_image)\n",
    "            search_results = search_multion(description, cropped_image)\n",
    "\n",
    "\n",
    "            search_res_imgs = [result.replace('(', '').replace(')', '').replace('\\n', '')[1] for result in search_results.message.split(',') if result]\n",
    "\n",
    "            if search_results:\n",
    "                best_match = feedback_loop(cropped_image, search_res_imgs, get_image_embedding, clip_processor)\n",
    "                if best_match:\n",
    "                    print(f\"Best match for {item['name']}: {best_match['link']}\")\n",
    "                else:\n",
    "                    print(f\"No suitable match found for {item['name']}\")\n",
    "            else:\n",
    "                print(f\"No search results found for {item['name']}\")\n",
    "        else:\n",
    "            print(f\"No object found for {item['name']}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T13:33:03.944607300Z",
     "start_time": "2024-06-07T13:33:03.895191500Z"
    }
   },
   "id": "5dde9cede360b65c",
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
